import numpy as np
import os
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import Dense, Input, Dropout,Flatten, Conv2D
from tensorflow.keras.layers import BatchNormalization, Activation, MaxPooling2D
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras.utils import plot_model
from IPython.display import SVG, Image
from livelossplot.inputs.tf_keras import PlotLossesCallback
import tensorflow as tf
import json
import numpy as np
from tensorflow.keras.models import model_from_json
import cv2
print("Tensorflow version:", tf.__version__)

class FacialExpressionModel(object):
    EMOTIONS_LIST = ["Angry", "Disgust", "Fear", "Happy", "Neutral", "Sad", "Surprise"]

    def __init__(self, model_json_file, model_weights_file):
        # Load model from JSON file
        with open(model_json_file, "r") as json_file:
            loaded_model_json = json.load(json_file)
            self.loaded_model = model_from_json(json.dumps(loaded_model_json))
        
        # Load weights into the new model
        self.loaded_model.load_weights(model_weights_file)

    def predict_emotion(self, img):
        preds = self.loaded_model.predict(img)
        return FacialExpressionModel.EMOTIONS_LIST[np.argmax(preds)]

facec = cv2.CascadeClassifier('face_model/haarcascade_frontalface_default.xml')
model = FacialExpressionModel("face_model/model.json", "face_model/model_weights.h5")
font = cv2.FONT_HERSHEY_SIMPLEX
class VideoCamera(object):
    def __init__(self):
        self.video = cv2.VideoCapture(0)
        self.pred = None
        self.pred_buffer = [None for _ in range(25)]
    def __del__(self):
        self.video.release()
    # returns camera frames along with bounding boxes and predictions
    def get_frame(self):
        _, fr = self.video.read()
        gray_fr = cv2.cvtColor(fr, cv2.COLOR_BGR2GRAY)
        faces = facec.detectMultiScale(gray_fr, 1.3, 5)
        for (x, y, w, h) in faces:
            fc = gray_fr[y:y+h, x:x+w]
            roi = cv2.resize(fc, (48, 48))
            self.pred = model.predict_emotion(roi[np.newaxis, :, :, np.newaxis])
            cv2.putText(fr, self.pred, (x, y), font, 1, (255, 255, 0), 2)
            cv2.rectangle(fr,(x,y),(x+w,y+h),(255,0,0),2)

        self.pred_buffer.append(self.pred)
        _ = self.pred_buffer.pop(0)
        return fr
    
def face_gen(camera):
    while True:
        frame = camera.get_frame()
        cv2.imshow('Facial Expression Recognization',frame)
        cv2.resizeWindow('Facial Expression Recognization', 500, 400)  # Set the desired size (width, height)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
    cv2.destroyAllWindows()



# if __name__ == '__main__':
#     face_gen(VideoCamera())